import os
import sys
import logging
from dataclasses import dataclass
from typing import Tuple
import csv

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split

from NTF import NTF, NTFConfig


@dataclass
class NTFTrainerConfig:
    """Training config for NTF.

    NTF is now trained using CSV-based epsilon data (generated by `Trainer`)
    via `build_ntf_training_data` in NTF.py.
    """

    max_rate: int = 16
    epochs: int = 50
    batch_size: int = 512
    lr: float = 1e-3
    val_split: float = 0.1
    seed: int = 0
    log_every: int = 50


class QuadNTFDataset(Dataset):
    """Dataset reading per-quad features derived from two CSV files.

    Targets are 4 continuous edge rates; we normalize to [0,1]
    by dividing by `max_rate`. Epsilon is standardized using mean/std
    computed over the whole dataset so that its scale is comparable to
    positions before being fed into the positional encoder.
    """

    def __init__(
            self,
            points: np.ndarray,
            quads: np.ndarray,
            s_bottom,
            s_right,
            s_top,
            s_left,
            epsilon,
            max_rate: int,
    ):
        super().__init__()
        self.points = points.astype(np.float32)
        self.quads = quads.astype(np.int64)
        self.max_rate = max_rate

        Q = self.quads.shape[0]

        def _broadcast_to_array(x, name: str):
            x_arr = np.asarray(x)
            if x_arr.shape == ():
                x_arr = np.full((Q,), x_arr, dtype=np.float32)
            elif x_arr.shape[0] != Q:
                raise ValueError(f"{name} length {x_arr.shape[0]} != num quads {Q}")
            return x_arr.astype(np.float32)

        def _to_float_array(x, name: str):
            arr = np.asarray(x, dtype=np.float32)
            if arr.shape == ():
                arr = np.full((Q,), float(arr), dtype=np.float32)
            if arr.shape[0] != Q:
                raise ValueError(f"{name} length {arr.shape[0]} != num quads {Q}")
            return arr

        self.s_bottom = _to_float_array(s_bottom, "sample_rate_bottom")
        self.s_right = _to_float_array(s_right, "sample_rate_right")
        self.s_top = _to_float_array(s_top, "sample_rate_top")
        self.s_left = _to_float_array(s_left, "sample_rate_left")

        # store raw epsilon and compute normalization stats
        self.epsilons_raw = _broadcast_to_array(epsilon, "epsilon")
        self.epsilon_mean = float(self.epsilons_raw.mean())
        self.epsilon_std = float(self.epsilons_raw.std() + 1e-8)

        if self.quads.ndim != 2 or self.quads.shape[1] != 4:
            raise ValueError(f"quads must have shape (Q,4), got {self.quads.shape}")

        if np.any(self.quads < 0) or np.any(self.quads >= self.points.shape[0]):
            raise ValueError("quad vertex indices out of range for points array")

    def __len__(self) -> int:
        return self.quads.shape[0]

    def __getitem__(self, idx: int):
        v_ids = self.quads[idx]
        verts = self.points[v_ids]
        eps_raw = float(self.epsilons_raw[idx])

        # epsilon normalization: standardize using dataset-level mean/std
        eps_norm = (eps_raw - self.epsilon_mean) / self.epsilon_std

        # build input feature: 4*3 coords + 1 normalized epsilon
        geom = verts.astype(np.float32).reshape(-1)
        feat = np.concatenate([geom, np.array([eps_norm], dtype=np.float32)], axis=0)
        x = torch.from_numpy(feat)

        # continuous target rates, normalized to [0,1] by max_rate
        rates = np.array([
            self.s_bottom[idx],
            self.s_right[idx],
            self.s_top[idx],
            self.s_left[idx],
        ], dtype=np.float32)
        y = torch.from_numpy(rates / float(self.max_rate))

        return x, y


class NTFTrainer:
    """Trainer using two CSVs -> NTF training data.

    Usage:
      python Trainer.py <points_csv> <quads_csv>

    where:
      - <points_csv> and <quads_csv> are produced by `generate_quad_training_csv`.
    """

    def __init__(self, points_csv: str, quads_csv: str, ntf_config: NTFConfig, trainer_cfg: NTFTrainerConfig):
        self.points_csv = points_csv
        self.quads_csv = quads_csv
        self.ntf_config = ntf_config
        self.config = trainer_cfg

        torch.manual_seed(self.config.seed)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        # Build arrays directly from two CSVs (points + Pareto quads).
        # points_csv: generated by generate_quad_training_csv, columns: point_id,x,y,z
        # quads_csv:  Pareto/prefix-optimal CSV, columns:
        #   quad_id,v0,v1,v2,v3,epsilon_target,sample_rate_bottom,sample_rate_right,sample_rate_top,sample_rate_left

        # Load points
        pts_list = []
        with open(points_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                pts_list.append((float(row["x"]), float(row["y"]), float(row["z"])))
        points = np.array(pts_list, dtype=np.float32)

        # Load quads and per-quad rates/epsilon
        quads = []
        s_bottom = []
        s_right = []
        s_top = []
        s_left = []
        eps = []

        with open(quads_csv, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            # Support both epsilon_target (Pareto CSV) and epsilon (full CSV) column names.
            has_eps_target = "epsilon_target" in (reader.fieldnames or [])
            has_eps = "epsilon" in (reader.fieldnames or [])
            if not (has_eps_target or has_eps):
                raise KeyError("quads CSV must have either 'epsilon_target' or 'epsilon' column")

            for row in reader:
                v0 = int(row["v0"])
                v1 = int(row["v1"])
                v2 = int(row["v2"])
                v3 = int(row["v3"])
                quads.append((v0, v1, v2, v3))

                sb = int(row["sample_rate_bottom"])
                sr = int(row["sample_rate_right"])
                st = int(row["sample_rate_top"])
                sl = int(row["sample_rate_left"])

                # Clamp to [1, max_rate] just in case
                max_r = self.config.max_rate
                sb = max(1, min(max_r, sb))
                sr = max(1, min(max_r, sr))
                st = max(1, min(max_r, st))
                sl = max(1, min(max_r, sl))

                s_bottom.append(sb)
                s_right.append(sr)
                s_top.append(st)
                s_left.append(sl)

                if has_eps_target:
                    eps_val = float(row["epsilon_target"])
                else:
                    eps_val = float(row["epsilon"])
                eps.append(eps_val)

        quads = np.array(quads, dtype=np.int64)
        s_bottom = np.array(s_bottom, dtype=np.int64)
        s_right = np.array(s_right, dtype=np.int64)
        s_top = np.array(s_top, dtype=np.int64)
        s_left = np.array(s_left, dtype=np.int64)
        eps = np.array(eps, dtype=np.float32)

        self.dataset = QuadNTFDataset(
            points=points,
            quads=quads,
            s_bottom=s_bottom,
            s_right=s_right,
            s_top=s_top,
            s_left=s_left,
            epsilon=eps,
            max_rate=self.config.max_rate,
        )

        # save epsilon normalization stats from dataset for later use (e.g., prediction)
        self.epsilon_mean = self.dataset.epsilon_mean
        self.epsilon_std = self.dataset.epsilon_std

        # Split dataset
        val_len = int(len(self.dataset) * self.config.val_split)
        train_len = len(self.dataset) - val_len
        train_set, val_set = random_split(self.dataset, [train_len, val_len])

        self.train_loader = DataLoader(
            train_set, batch_size=self.config.batch_size, shuffle=True, drop_last=False
        )
        self.val_loader = DataLoader(
            val_set, batch_size=self.config.batch_size, shuffle=False, drop_last=False
        )

        # Raw feature: 4*3 coords + 1 epsilon
        in_dim_raw = 4 * 3 + 1
        self.ntf = NTF(in_dim_raw=in_dim_raw, config=self.ntf_config).to(self.device)

        # Regression over 4 normalized edge rates
        self.criterion = nn.MSELoss()
        self.optimizer = torch.optim.Adam(self.ntf.parameters(), lr=self.config.lr)

        base, _ = os.path.splitext(self.quads_csv)
        self.model_path = base + '_ntf.pt'

    def run(self) -> None:
        """Main training loop for NTF (regression)."""
        best_val = float('inf')
        global_step = 0

        for epoch in range(1, self.config.epochs + 1):
            self.ntf.train()
            running_loss = 0.0

            for x, y in self.train_loader:
                x = x.to(self.device)
                y = y.to(self.device)  # (B, 4) normalized rates

                self.optimizer.zero_grad()
                pred = self.ntf(x)  # (B, 4)
                loss = self.criterion(pred, y)
                loss.backward()
                self.optimizer.step()

                running_loss += loss.item() * x.size(0)

                if global_step % self.config.log_every == 0:
                    logging.info(
                        f'[NTF-Trainer] epoch={epoch} step={global_step} loss={loss.item():.6f}'
                    )
                global_step += 1

            avg_train = running_loss / max(1, len(self.train_loader.dataset))

            # Validation
            self.ntf.eval()
            val_loss = 0.0
            total = 0
            # accumulate mean absolute error (MAE) after denormalizing rates
            abs_err_sum = 0.0

            with torch.no_grad():
                for x, y in self.val_loader:
                    x = x.to(self.device)
                    y = y.to(self.device)

                    pred = self.ntf(x)
                    l = self.criterion(pred, y)
                    val_loss += l.item() * x.size(0)

                    # metric: absolute error after denormalizing to real rates
                    pred_denorm = pred * float(self.config.max_rate)
                    y_denorm = y * float(self.config.max_rate)
                    # per-sample MAE over 4 edges
                    abs_err = (pred_denorm - y_denorm).abs().mean(dim=1)
                    abs_err_sum += abs_err.sum().item()
                    total += x.size(0)

            avg_val = val_loss / max(1, len(self.val_loader.dataset))
            mean_abs_err = abs_err_sum / max(1, total)

            logging.info(
                f'[NTF-Trainer] epoch={epoch}/{self.config.epochs} '
                f'train_loss={avg_train:.6f} val_loss={avg_val:.6f} '
                f'mean_abs_err={mean_abs_err:.3f}'
            )

            if avg_val < best_val:
                best_val = avg_val
                self._save()
                logging.info(
                    f'[NTF-Trainer] saved best model (val_loss={best_val:.6f}) -> {self.model_path}'
                )

        logging.info('[NTF-Trainer] training finished.')

    def _save(self) -> None:
        torch.save(
            {
                'state_dict': self.ntf.state_dict(),
                'ntf_config': self.ntf_config.__dict__,
                'trainer_config': self.config.__dict__,
                # save epsilon normalization so prediction can reuse it
                'epsilon_mean': self.epsilon_mean,
                'epsilon_std': self.epsilon_std,
            },
            self.model_path,
        )

    def load(self) -> None:
        if not os.path.isfile(self.model_path):
            raise FileNotFoundError(self.model_path)
        data = torch.load(self.model_path, map_location=self.device)

        # restore epsilon normalization stats
        self.epsilon_mean = data.get('epsilon_mean', 0.0)
        self.epsilon_std = data.get('epsilon_std', 1.0)

        # re-create NTF with saved config
        ntf_cfg = NTFConfig(**data['ntf_config'])
        self.ntf = NTF(in_dim_raw=4 * 3 + 1, config=ntf_cfg).to(self.device)
        self.ntf.load_state_dict(data['state_dict'])
        self.ntf.eval()

    def predict(self, quad_vertices: np.ndarray, epsilon: float) -> Tuple[float, float, float, float]:
        """Predict four continuous edge rates for a single quad.

        Returns (bottom, right, top, left) as floats in [1, max_rate] after
        denormalization and clamping.
        """
        if not hasattr(self, 'ntf') or self.ntf is None:
            self.load()

        # apply the same epsilon normalization used during training
        eps_norm = (float(epsilon) - float(self.epsilon_mean)) / float(self.epsilon_std)

        geom = quad_vertices.astype(np.float32).reshape(-1)
        feat = np.concatenate([geom, np.array([eps_norm], dtype=np.float32)], axis=0)
        x = torch.from_numpy(feat).unsqueeze(0).to(self.device)

        with torch.no_grad():
            pred_norm = self.ntf(x)[0]  # (4,)

        # denormalize and clamp to [1, max_rate]
        rates = pred_norm * float(self.config.max_rate)
        rates = torch.clamp(rates, 1.0, float(self.config.max_rate))
        b, r, t, l = rates.tolist()
        return b, r, t, l


if __name__ == '__main__':
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)],
    )

    points_csv = "assets/Icosphere_baked_points.csv"
    # Use the Pareto / prefix-optimal CSV generated by generate_quad_training_csv
    # so that for each quad and epsilon_target we have the best rate combination
    # seen up to that epsilon.
    quads_csv = "assets/Icosphere_baked_quads_pareto.csv"

    ntf_cfg = NTFConfig(fflevels=8, hidden_dim=64, max_rate=4)
    trainer_cfg = NTFTrainerConfig(max_rate=4, epochs=50, batch_size=1024, lr=5e-4)

    trainer = NTFTrainer(points_csv, quads_csv, ntf_cfg, trainer_cfg)
    trainer.run()

    mode = "predict"
    if mode == "predict":
        trainer = NTFTrainer(points_csv, quads_csv, ntf_cfg, trainer_cfg)

        trainer.load()
        logging.info(f"Loaded model from: {trainer.model_path}")
        logging.info(f"epsilon_mean={trainer.epsilon_mean:.6f}, epsilon_std={trainer.epsilon_std:.6f}")

        num_samples = 5
        num_quads = len(trainer.dataset)
        rng = np.random.default_rng(seed=0)

        for _ in range(num_samples):
            idx = int(rng.integers(low=0, high=num_quads))
            quad_idx = trainer.dataset.quads[idx]
            quad_vertices = trainer.dataset.points[quad_idx]
            epsilon = float(trainer.dataset.epsilons_raw[idx])

            b, r, t, l = trainer.predict(quad_vertices, epsilon)

            gt_bottom = trainer.dataset.s_bottom[idx]
            gt_right = trainer.dataset.s_right[idx]
            gt_top = trainer.dataset.s_top[idx]
            gt_left = trainer.dataset.s_left[idx]

            print("---- Sample idx:", idx)
            print("quad vertex ids:", quad_idx.tolist())
            print("epsilon:", epsilon)
            print("GT rates      (b, r, t, l):",
                  gt_bottom, gt_right, gt_top, gt_left)
            print("Predicted rates (b, r, t, l):",
                  f"{b:.3f}", f"{r:.3f}", f"{t:.3f}", f"{l:.3f}")
    else:
        raise ValueError(f"Unknown mode: {mode}")
